# Interpreting_the_blackbox_Explanations
### Discription

This paper is published at healthinfo2025 conference (https://www.scitepress.org/Papers/2025/131574/131574.pdf)

Abstract:
Explainable AI (XAI) assist clinicians and researcher in understanding the rationale behind the predictions
made by data-driven models which helps them to make informed decisions and trust the modelâ€™s outputs.
Providing accurate explanations for breast cancer treatment predictions in the context of highly imbalanced,
multiclass-multioutput classification problem is extremely challenging. The aim of this study is to perform
a comprehensive and detailed analysis of the explanations generated by post-hoc explanatory methods: Local Interpretable Model-agnostic Explanation (LIME) and SHaply Additive exPlanations (SHAP) for breast
cancer treatment prediction using highly imbalanced oncologycal dataset. We introduced evaluation matrices including consistency, fidelity, alignment with established clinical guidelines and qualitative analysis to
evaluate the effectiveness and faithfulness of these methods. By examining the strengths and limitations of
LIME and SHAP, we aim to determine their suitability for supporting clinical decision making in multifaceted
treatments and complex scenarios. Our findings provide important insights into the use of these explanation
methods, highlighting the importance of transparent and robust predictive models. This experiment showed
that SHAP perform better than LIME in term of fidelity and by providing more stable explanation that are
better aligned with medical guidelines. This work provides guidance to practitioners and model developers
in selecting the most suitable explanation technique to promote trust and enhance understanding in predictive
healthcare models.

### Code Explanation

There are three files in the project directory.
The main.py file is responsible for running experiments and saving results, serving as the entry point of the workflow.

The explainer.py module provides model interpretability tools using SHAP and LIME.
It includes functions to generate local feature explanations, compute concordance between explanations, and calculate stability metrics such as the Variance Score Index (VSI).

The utils.py module handles data loading and preprocessing, integrates relevant medical guidelines, and trains a Random Forest classifier.
It also provides functions to evaluate LIME and SHAP explanations across fidelity, stability, and guideline-based comparisons, enabling structured assessment of model interpretability.

The logging_config.py module manages logging setup, ensuring that key steps, warnings, and errors are properly recorded for traceability.


### Running instruction 
Place the data file in the repository.
Update the dataset_path in the project/main.py file under ( __name__ == "__main__" ) function.
Run main.py.
It will:
    (a) creat a logs directory in the repository which tell all the running states of the program.
    (b) creat a results directory and store the comparison plots and results datafram in the it.
