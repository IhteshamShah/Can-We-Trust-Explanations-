
# Can We Trust Explanations! 
**Evaluation of Model-Agnostic Explanation Techniques on Highly**
**Imbalanced, Multiclass-Multioutput Classification Problem**

This Paper is published at HealthInfo2025 conference (https://www.scitepress.org/Papers/2025/131574/131574.pdf)

This repository contains the implementation of experiments evaluating the **trustworthiness of machine learning explanations** in the medical domain.
We focus on **LIME** and **SHAP** explainers, and evaluate them along three key dimensions:

* **Fidelity** ‚Äì how well the explanation reflects the model‚Äôs prediction.
* **Stability** ‚Äì how consistent the explanation is across repeated runs.
* **Medical Guidelines Concordance** ‚Äì how well the explanations align with expert medical knowledge.

---

## üìÇ Project Structure

* **`main.py`** ‚Äì Runs the experiments, executes explanation methods, and saves results.
* **`utils.py`** ‚Äì Provides data loading, preprocessing, integration of medical guidelines, Random Forest training, and functions to compute explanation results across fidelity, stability, and guideline comparison.
* **`explainer.py`** ‚Äì Implements **LIME** and **SHAP** explainers, functions to extract top contributing features, and methods for computing variance/stability indices.
* **`logging_config.py`** ‚Äì Centralized logging configuration for capturing experiment outputs, warnings, and errors.
* **`config.yaml`** ‚Äî Central configuration file for all parameters.
---

**Run the code**
1. Edit **config.yaml** to set dataset paths, model parameters, explainer settings, and plotting options. (**recommended= keep all the setting same just update the dataset paths*)
2. Nevigate to the project directory ``` cd project.py ``` and Run the main script: ```bash python main.py ```
3. **Check results and plots** in the **results/* directory.
4. Review logs in project.log for progress and debugging.

## Configuration
All parameters are defined in config.yaml and grouped by module:

```bash
explainer:
  shap: {...}
  lime: {...}
  vsi: {...}
main:
  treatments: {...}
  data: {...}
  plots: {...}
utils:
  data: {...}
  preprocessing: {...}
  smote: {...}
  random_forest: {...}
  guidelines: {...}
plots: {...}
```

## ‚öôÔ∏è Installation

```bash
# Clone the repository
git clone https://github.com/IhteshamShah/Can-We-Trust-Explanations-.git
cd project

# Install dependencies
pip install -r requirements.txt
```

---

## üöÄ Usage

Run the experiments with:

```bash
python main.py
```

Results (logs and plots) are saved automatically to the **`logs/project.log`** and  **`projects/results/`** directory.

---

## üìä Results

The framework provides:

* **Plots** comparing SHAP and LIME explanations.
* **CSV outputs** containing treatment-level fidelity, stability, and guideline concordance results.
* **Logs** recording each step of the experimental workflow.

---

## üßæ Citation

If you use this repository for your research, please cite:

```
@article{shahcan,
  title={Can We Trust Explanation! Evaluation of Model-Agnostic Explanation Techniques on Highly Imbalanced, Multiclass-Multioutput Classification Problem},
  author={Shah, Syed Ihtesham Hussain and ten Teije, Annette and Volders, Jos{\'e}}
}
```

---

## üìå License

This project is licensed under the MIT License ‚Äì see the [LICENSE](LICENSE) file for details.