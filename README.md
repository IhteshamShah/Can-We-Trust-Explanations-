
# Can We Trust Explanations! 
**Evaluation of Model-Agnostic Explanation Techniques on Highly**
**Imbalanced, Multiclass-Multioutput Classification Problem**

This Paper is published at HealthInfo2025 conference (https://www.scitepress.org/Papers/2025/131574/131574.pdf)

This repository contains the implementation of experiments evaluating the **trustworthiness of machine learning explanations** in the medical domain.
We focus on **LIME** and **SHAP** explainers, and evaluate them along three key dimensions:

* **Fidelity** ‚Äì how well the explanation reflects the model‚Äôs prediction.
* **Stability** ‚Äì how consistent the explanation is across repeated runs.
* **Medical Guidelines Concordance** ‚Äì how well the explanations align with expert medical knowledge.

---

## üìÇ Project Structure

* **`main.py`** ‚Äì Runs the experiments, executes explanation methods, and saves results.
* **`utils.py`** ‚Äì Provides data loading, preprocessing, integration of medical guidelines, Random Forest training, and functions to compute explanation results across fidelity, stability, and guideline comparison.
* **`explainer.py`** ‚Äì Implements **LIME** and **SHAP** explainers, functions to extract top contributing features, and methods for computing variance/stability indices.
* **`logging_config.py`** ‚Äì Centralized logging configuration for capturing experiment outputs, warnings, and errors.

---

## ‚öôÔ∏è Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/can-we-trust-explanation.git
cd can-we-trust-explanation

# Create and activate environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
```

---

## üöÄ Usage

Run the experiments with:

```bash
python main.py
```

Results (plots and logs) are saved automatically to the **`models/workspace/Explanation_plot/`** directory.
Example output includes SHAP and LIME explanation plots with and without treatment parameters:

```
survival_prediction_12m_no_treatment_Explanation.png
survival_prediction_12m_with_treatment_Explanation.png
```

---

## üìä Results

The framework provides:

* **Plots** comparing SHAP and LIME explanations.
* **CSV outputs** containing treatment-level fidelity, stability, and guideline concordance results.
* **Logs** recording each step of the experimental workflow.

---

## üßæ Citation

If you use this repository for your research, please cite:

```
@article{shahcan,
  title={Can We Trust Explanation! Evaluation of Model-Agnostic Explanation Techniques on Highly Imbalanced, Multiclass-Multioutput Classification Problem},
  author={Shah, Syed Ihtesham Hussain and ten Teije, Annette and Volders, Jos{\'e}}
}
```

---

## üìå License

This project is licensed under the MIT License ‚Äì see the [LICENSE](LICENSE) file for details.







# Can We Trust Explanation? Interpreting_the_Explanation techniques in machine-learning
### Discription

This paper is published at healthinfo2025 conference (https://www.scitepress.org/Papers/2025/131574/131574.pdf)

Abstract:
Explainable AI (XAI) assist clinicians and researcher in understanding the rationale behind the predictions
made by data-driven models which helps them to make informed decisions and trust the model‚Äôs outputs.
Providing accurate explanations for breast cancer treatment predictions in the context of highly imbalanced,
multiclass-multioutput classification problem is extremely challenging. The aim of this study is to perform
a comprehensive and detailed analysis of the explanations generated by post-hoc explanatory methods: Local Interpretable Model-agnostic Explanation (LIME) and SHaply Additive exPlanations (SHAP) for breast
cancer treatment prediction using highly imbalanced oncologycal dataset. We introduced evaluation matrices including consistency, fidelity, alignment with established clinical guidelines and qualitative analysis to
evaluate the effectiveness and faithfulness of these methods. By examining the strengths and limitations of
LIME and SHAP, we aim to determine their suitability for supporting clinical decision making in multifaceted
treatments and complex scenarios. Our findings provide important insights into the use of these explanation
methods, highlighting the importance of transparent and robust predictive models. This experiment showed
that SHAP perform better than LIME in term of fidelity and by providing more stable explanation that are
better aligned with medical guidelines. This work provides guidance to practitioners and model developers
in selecting the most suitable explanation technique to promote trust and enhance understanding in predictive
healthcare models.

### Code Explanation

There are three files in the project directory.
The main.py file is responsible for running experiments and saving results, serving as the entry point of the workflow.

The explainer.py module provides model interpretability tools using SHAP and LIME.
It includes functions to generate local feature explanations, compute concordance between explanations, and calculate stability metrics such as the Variance Score Index (VSI).

The utils.py module handles data loading and preprocessing, integrates relevant medical guidelines, and trains a Random Forest classifier.
It also provides functions to evaluate LIME and SHAP explanations across fidelity, stability, and guideline-based comparisons, enabling structured assessment of model interpretability.

The logging_config.py module manages logging setup, ensuring that key steps, warnings, and errors are properly recorded for traceability.


### Running instruction 
Place the data file in the repository.
Update the dataset_path in the project/main.py file under ( __name__ == "__main__" ) function.
Run main.py.
It will:
    (a) creat a logs directory in the repository which tell all the running states of the program.
    (b) creat a results directory and store the comparison plots and results datafram in the it.
